{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "[[0.5088523]]\n",
      "[[0.49920043]]\n",
      "[[0.5104897]]\n",
      "[[0.50842905]]\n",
      "[[0.5151948]]\n",
      "[[0.5405941]]\n",
      "[[0.5643462]]\n",
      "[[0.5446451]]\n",
      "[[0.511352]]\n",
      "[[0.51998687]]\n",
      "[[0.49892744]]\n",
      "[[0.4854251]]\n",
      "[[0.5154675]]\n",
      "[[0.53059345]]\n",
      "[[0.5155532]]\n",
      "[[0.46792305]]\n",
      "[[0.51950634]]\n",
      "[[0.5159077]]\n",
      "[[0.5416336]]\n",
      "[[0.53890324]]\n",
      "[[0.52203554]]\n",
      "[[0.5297162]]\n",
      "[[0.50373024]]\n",
      "[[0.5179671]]\n",
      "[[0.5281589]]\n",
      "[[0.5054146]]\n",
      "[[0.5110869]]\n",
      "[[0.51732033]]\n",
      "[[0.5106951]]\n",
      "[[0.5170113]]\n",
      "[[0.5124434]]\n",
      "[[0.51249826]]\n",
      "[[0.4988435]]\n",
      "[[0.5360318]]\n",
      "[[0.51405597]]\n",
      "[[0.5118872]]\n",
      "[[0.51142406]]\n",
      "[[0.51700574]]\n",
      "[[0.50053]]\n",
      "[[0.5144951]]\n",
      "[[0.50949633]]\n",
      "[[0.5187515]]\n",
      "[[0.5117243]]\n",
      "[[0.50005704]]\n",
      "[[0.51406884]]\n",
      "[[0.5129771]]\n",
      "[[0.51334536]]\n",
      "[[0.50303227]]\n",
      "[[0.5384573]]\n",
      "[[0.5098624]]\n",
      "[[0.5124483]]\n",
      "[[0.5145661]]\n",
      "[[0.5156941]]\n",
      "[[0.5119009]]\n",
      "[[0.51977986]]\n",
      "[[0.5173405]]\n",
      "[[0.5323397]]\n",
      "[[0.48755965]]\n",
      "[[0.50595504]]\n",
      "[[0.5033086]]\n",
      "[[0.51626164]]\n",
      "[[0.5202695]]\n",
      "[[0.5184417]]\n",
      "[[0.5280922]]\n",
      "[[0.523526]]\n",
      "[[0.5156605]]\n",
      "[[0.5173372]]\n",
      "[[0.51723987]]\n",
      "[[0.52701414]]\n",
      "[[0.5343968]]\n",
      "[[0.51272386]]\n",
      "[[0.51496935]]\n",
      "[[0.53606784]]\n",
      "[[0.5101372]]\n",
      "[[0.5264721]]\n",
      "[[0.5071362]]\n",
      "[[0.5144894]]\n",
      "[[0.5267312]]\n",
      "[[0.5379942]]\n",
      "[[0.52534735]]\n",
      "[[0.5182025]]\n",
      "[[0.5099002]]\n",
      "[[0.50980085]]\n",
      "[[0.5414939]]\n",
      "[[0.54031813]]\n",
      "[[0.52182466]]\n",
      "[[0.49696398]]\n",
      "[[0.5470353]]\n",
      "[[0.5484176]]\n",
      "[[0.5349034]]\n",
      "[[0.53613037]]\n",
      "[[0.5147863]]\n",
      "[[0.5372623]]\n",
      "[[0.52165097]]\n",
      "[[0.52064323]]\n",
      "[[0.52235544]]\n",
      "[[0.5403776]]\n",
      "[[0.5222421]]\n",
      "[[0.5077182]]\n",
      "[[0.5394354]]\n",
      "[[0.5091228]]\n",
      "[[0.52158195]]\n",
      "[[0.52468985]]\n",
      "[[0.52975434]]\n",
      "[[0.4936099]]\n",
      "[[0.52978164]]\n",
      "[[0.5125824]]\n",
      "[[0.54327977]]\n",
      "[[0.517597]]\n",
      "[[0.5173044]]\n",
      "[[0.51805925]]\n",
      "[[0.55092037]]\n",
      "[[0.5027252]]\n",
      "[[0.5521684]]\n",
      "[[0.48721197]]\n",
      "[[0.51139385]]\n",
      "[[0.5145228]]\n",
      "[[0.52363515]]\n",
      "[[0.5216519]]\n",
      "[[0.51043355]]\n",
      "[[0.5323829]]\n",
      "[[0.51163286]]\n",
      "[[0.49989542]]\n",
      "[[0.5141434]]\n",
      "[[0.49887267]]\n",
      "[[0.5218644]]\n",
      "[[0.5124933]]\n",
      "[[0.5006385]]\n",
      "[[0.52058864]]\n",
      "[[0.4930284]]\n",
      "[[0.5089371]]\n",
      "[[0.5100178]]\n",
      "[[0.539486]]\n",
      "[[0.4866321]]\n",
      "[[0.48850274]]\n",
      "[[0.54776204]]\n",
      "[[0.5305402]]\n",
      "[[0.54180396]]\n",
      "[[0.49404666]]\n",
      "[[0.48527166]]\n",
      "[[0.5261615]]\n",
      "[[0.5096967]]\n",
      "[[0.54470384]]\n",
      "('6cdd3723ccd244648362dbc1639f339f.png', 127, 16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import UNet\n",
    "import utils\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import image as Image\n",
    "from keras.applications.xception import preprocess_input\n",
    "import uuid\n",
    "\n",
    "localizationWeights = r'../checkpoints/MBM/epoch_10.pth.tar'\n",
    "classifyModel = r'../checkpoints/LUSC/my_model.h5'\n",
    "\n",
    "\n",
    "def detect(srcImgPath):\n",
    "    model = UNet(2).to(utils.device())\n",
    "    model.load_state_dict(torch.load(localizationWeights, map_location=utils.device())['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    image = utils.read_image(srcImgPath)\n",
    "    transform = T.Compose([T.ToTensor(), T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    x = transform(image).unsqueeze(0)\n",
    "    points = None\n",
    "    with torch.no_grad():\n",
    "        x = x.to(utils.device())\n",
    "        pred = utils.ensure_array(model(x)).squeeze(0)\n",
    "        points = utils.extract_points_from_direction_field_map(pred, lambda1=0.7, step=10)\n",
    "    \n",
    "    # save image and points\n",
    "    image_array = np.array(image)\n",
    "    plt.figure(dpi=500)\n",
    "    plt.imshow(image_array)\n",
    "    plt.axis('off')\n",
    "    points_array = np.array(points)\n",
    "    plt.plot(points_array[:, 1], points_array[:, 0], marker='o', markerfacecolor='#f9f738', markeredgecolor='none', markersize=2,\n",
    "             linestyle='none')\n",
    "    uid = str(uuid.uuid4())\n",
    "    fileName = ''.join(uid.split('-')) + \".png\"\n",
    "    savePath = 'F:/demo/res/img/' + fileName\n",
    "    plt.savefig(savePath, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return fileName, image, points\n",
    "\n",
    "def getPatch(image, point):\n",
    "    xMax, yMax = image.size\n",
    "    x, y = point\n",
    "    l = (x - 30) if ((x - 30) > 0) else 0\n",
    "    r = (x + 30) if ((x + 30) < xMax) else xMax\n",
    "    h = (y - 30) if ((y - 30) > 0) else 0\n",
    "    b = (y + 30) if ((y + 30) < yMax) else yMax\n",
    "    box = (l, h, r, b)\n",
    "\n",
    "    patch = image.crop(box)\n",
    "    return patch\n",
    "\n",
    "def classify(model ,image):\n",
    "    x = np.array(image)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    predict = model.predict(x)\n",
    "    print(predict)\n",
    "\n",
    "    if round(predict[0,0])==0:\n",
    "        result = 'ying'\n",
    "    else:\n",
    "        result = 'yang'\n",
    "\n",
    "    return result\n",
    "\n",
    "    \n",
    "\n",
    "def analysis(srcImgPath):\n",
    "    # 获取细胞位置\n",
    "    fileName, image, points = detect(srcImgPath)\n",
    "    # print(len(points))\n",
    "\n",
    "    model = keras.models.load_model(classifyModel)\n",
    "    yang = 0\n",
    "    ying = 0\n",
    "    for point in points:\n",
    "        patch = getPatch(image, point)\n",
    "        patch = tf.image.resize(patch, [80, 80])\n",
    "        patch = tf.cast(patch, tf.float32)\n",
    "        patch = patch / 255\n",
    "\n",
    "        result = classify(model, patch)\n",
    "        if result == 'yang':\n",
    "            yang += 1\n",
    "        else:\n",
    "            ying += 1\n",
    "    return fileName, yang, ying\n",
    "\n",
    "\n",
    "\n",
    "srcImgPath = 'F:/demo/res/img/359067147c464632b4c02d5ea777d44f.jpg'\n",
    "print(analysis(srcImgPath=srcImgPath))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "734a4ae29d77f7f2c472599c9986f1d209ec24a454c56d97796afe019c1300fa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
